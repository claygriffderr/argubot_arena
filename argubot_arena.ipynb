{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433ce339",
   "metadata": {},
   "source": [
    "# ArguBot Arena: Prompt Engineering a Debate on Responsible AI\n",
    "\n",
    "In this assignment you will configure an LLM through the use of prompts and system prompts to defend a position on a contentious/controversial issue around responsible AI usage.  \n",
    "\n",
    "This notebook is both a reference and where you will complete the assignment. As such, it is broken into several sections. \n",
    "\n",
    "The first two sections:\n",
    "1. Provide an introduction to using Ollama to load and interact with an LLM, and\n",
    "2. Show what a system prompt is and how it can be used to configure an LLM's behavior. \n",
    "\n",
    "After, in the next section you will:\n",
    "\n",
    "3. Write your own system prompt to have the LLM support only one position of a debate topic. \n",
    "Note that in order to _win_ the debate your LLM must stay on topic and not stray out of their role as a debater. For example, if the LLM tries to support both sides of the argument, or deviates in anyway, then they would lose (and you would lose points).\n",
    "\n",
    "Lastly, in the final section you will:\n",
    "\n",
    "4. Manage two LLMs(debaters), prompting both appropiately and maintaining context so that the two opponents can effectively respond to one another's arguments.\n",
    "\n",
    "__Note:__ \n",
    "It is important to recognize ahead of time that you will likely need to experiment with your prompts several times. In other words, this is not a notebook you will be able to run through quickly and execute each code cell just one time. You will likely need to run and re-run some code cells several times to see how the LLM behaves for your given prompt, then modify the prompt accordingly between each run. While this may seem tedious and unneccessary, it is the reality of working with LLMs and building LLM applications.\n",
    "\n",
    "If you are working locally, already have Ollama installed, as well as the LLM(s) you want to use, then you can jump to Step 1. below (code cell with `import ollama`). If you are usign Colab or another online platform, then click below and go to Step 0. to install ollama and start the Ollama server. \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MSU-CS3120/argubot_arena/blob/main/argubot_arena.ipynb)\n",
    "\n",
    "--- \n",
    "\n",
    "### 0. Install Ollama and download chosen LLM(s)\n",
    "\n",
    "Some of this section may seem unfamiliar to you because when using Jupyter notebooks we usually don't need to execute anything outside of the notebook itself. In this case though, because Ollama runs using a client/server model, you'll need to set up and access a terminal. So first run this code cell to enable a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6667cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20b199",
   "metadata": {},
   "source": [
    "To start the terminal run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205142e6",
   "metadata": {},
   "source": [
    "Once the terminal is running, you will need to run the two commands (from within the terminal window). It is best to run these separately, starting with this line:\n",
    "\n",
    "`curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "Then this one:\n",
    "\n",
    "`ollama serve &`\n",
    "\n",
    "You will then need to select a model to download (if working on Colab, then it is recommended that you stick to smaller models, i.e. less than 2B or 3B - here is the [list of Ollama models available](https://ollama.com/search)). \n",
    "\n",
    "`ollama pull llama3.2:1b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4037d",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 1. Using Ollama\n",
    "To start using Ollama we'll need to import the Python ollama module. As a simple example, you'll then create a simple prompt and generate the response. \n",
    "\n",
    "Note that if you are using Ollama for the first time, or using Google Colab, then you will need to uncomment the following cell and run this first in order to download and install the Python ollama module before importing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86527225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028af342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "debater1 = \"llama3.2:1b\"\n",
    "debater2 = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da2b31",
   "metadata": {},
   "source": [
    "This next formatting function will be used later to help us view the LLM output in a clean and consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def format_output(text, max_width=100):\n",
    "    cleaned_text = text.replace('\\n', ' ') # remove newlines and extra spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    wrapped_text = textwrap.fill(cleaned_text, width=max_width)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01eef0c",
   "metadata": {},
   "source": [
    "Next, let's define the question related to Responsible AI that we will consider. \n",
    "Without any system prompts or configuration, we'll then ask an LLM to answer our contentious/controversial question. \n",
    "\n",
    "Note: Before doing this you may need to download the LLM you are using. Below we are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama.pull(debater1)\n",
    "question = 'Should artificial intelligence be used to create fully autonomous military weapons?'\n",
    "\n",
    "response = ollama.chat(model=debater1, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1d82",
   "metadata": {},
   "source": [
    "If you saw an error above because your chosen model is not available on your system or Colab instance, then uncomment the line of code, `ollama.pull(debater1)`, and rerun the cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb717b9",
   "metadata": {},
   "source": [
    "--- \n",
    "### 2. Configuring an LLM with a System Prompt\n",
    "Next, let's see how a [system\n",
    "prompt](https://promptengineering.org/system-prompts-in-large-language-models/)\n",
    "can be used to modify the behavior of the LLM. \n",
    "\n",
    "In this fun example we'll simply ask the LLM to respond as a pirate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an AI assistant that always speaks like a pirate.',\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d6403",
   "metadata": {},
   "source": [
    "Note that system prompts can be\n",
    "longer and more sophisticated than that. For example, Anthropic has made their\n",
    "system prompts available, here: \n",
    "* https://docs.anthropic.com/en/release-notes/system-prompts\n",
    "\n",
    "Looking at any of those, it's easy to see that system prompts can be very\n",
    "detailed and specific. \n",
    "\n",
    "Although probably not as detailed as Claude's, you will need to create a \n",
    "system prompt that encourages the LLM to behave as an effective debater.\n",
    "\n",
    "--- \n",
    "\n",
    "### 3. Experimenting with your own System Prompt\n",
    "Now it is your turn to create a system prompt for the opening round of the debate. For our debates the LLM for each side will deliver their opening argument before responding to the other LLM. Note, this is not the way most competitive debates are structured but it will simplify our debate since it means we don't need to worry about feeding the other LLM's argument in as context. In 4. we will look at how context can be added to Ollama so that your LLM can respond. \n",
    "\n",
    "You'll likely want to experiment with this quite a bit and consider adding explicit directions to the system prompt that the LLM should follow. \n",
    "\n",
    "Specifically, __your system prompt should have the LLM:__\n",
    "* __only argue only _for_ OR _against_ the contentious question you chose (or were assigned), but not both sides__, and\n",
    "* __stay in its role as a debater and not stray outside of this role__. \n",
    "\n",
    "Remember how long the Claude [system prompt](https://docs.anthropic.com/en/release-notes/system-prompts) was. Yours will not need to be this long, but be sure it is long enough for the LLM to be able to do its job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_question = # WRITE THE QUESTION YOU SELECTED HERE (OR COPY AND PASTE THE ONE THAT WAS ASSIGNED TO YOU)\n",
    "your_sys_prompt = # YOUR SYSTEM PROMPT (YOU WILL NEED TO WRITE THIS AND EXPERIMENT WITH THIS UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': your_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(response['message']['content']) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b1198",
   "metadata": {},
   "source": [
    "Again, you will likely need to experiment with your prompt above and re-run the cell several times. \n",
    "\n",
    "The __LLM output should not deviate from its role as a debater__, and it __should make a strong argument for the side it has been assigned__.\n",
    "\n",
    "--- \n",
    "\n",
    "### 4. Running the Debate between Opponents (and maintaining context)\n",
    "\n",
    "Now, you're ready to try and coordinate the debate between the two opponents. Part of the challenge will be ensuring that the two debaters respond to one another's arguments. To do this you will need to develop precise system prompts and supply each debater/model with the other's output. \n",
    "\n",
    "Specifically, the debater/model going second will need to respond to the\n",
    "argument made by the one that went first. The debaters/models must stay in their role this entire time. \n",
    "\n",
    "Below is an example of what this second round might look like. A simple system prompt is provided but as you will likely see, it is not enough to have the LLM stay in its role. Notice that `debater2` is now used, which means that you may find it useful to also use distinct system prompts for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fe06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_question =  # WRITE THE QUESTION YOU SELECTED HERE (OR COPY AND PASTE THE ONE THAT WAS ASSIGNED TO YOU)\n",
    "debater1_sys_prompt = # YOUR SYSTEM PROMPT FOR DEBATER 1 (YOU WILL NEED TO WRITE THIS AND EXPERIMENT WITH THIS UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=debater1, options=dict(seed=100), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater1_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "\n",
    "debater1_opening_argument = response['message']['content']\n",
    "print(f\"****** Debater 1 Opening Argument ******\\n\")\n",
    "print(format_output(debater1_opening_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama.pull(debater2)\n",
    "\n",
    "debater2_sys_prompt = # YOUR SYSTEM PROMPT FOR DEBATER 2 (YOU WILL NEED TO WRITE THIS AND EXPERIMENT WITH THIS UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=debater2, options=dict(seed=2), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater2_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Opponent's Opening Argument:\\n\" + debater1_opening_argument,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "\n",
    "debater2_opening_argument = response['message']['content']\n",
    "print(f\"****** Debater 2 Opening Argument ******\\n\")\n",
    "print(format_output(debater2_opening_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea065b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "debater1_sys_prompt = # YOUR SYSTEM PROMPT FOR DEBATER 1 FOR ROUND 2 (YOU WILL NEED TO WRITE THIS AND EXPERIMENT WITH THIS UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater1_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Your Opening Argument:\\n\" + debater1_opening_argument,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Opponent's Opening Argument:\\n\" + debater2_opening_argument,\n",
    "    },\n",
    "])\n",
    "\n",
    "debater1_closing_argument = response['message']['content']\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(debater1_closing_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "debater2_sys_prompt = # YOUR SYSTEM PROMPT FOR DEBATER 2 FOR ROUND 2 (YOU WILL NEED TO WRITE THIS AND EXPERIMENT WITH THIS UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=debater2, options=dict(seed=2), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater2_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Your Opponent's Opening Argument:\\n\" + debater1_opening_argument,\n",
    "    },\n",
    "\n",
    "    ...\n",
    "    \n",
    "])\n",
    "\n",
    "debater2_closing_argument = response['message']['content']\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(debater2_closing_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1a026",
   "metadata": {},
   "source": [
    "### 5. (Optional) Synthesize the Debate Results into Speeches\n",
    "\n",
    "If you want to, try taking all of the debate speeches and synthesizing them into audio. The code below uses a simple text-to-speech Python library that, unfortunately, does not allow for different voices, but still provides an output mp3 with the full debate that you can later listen to. There are more advanced TTS modules/tools that do allow this, if you want to utilize though please feel free to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6963ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f323186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d329fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full debate text\n",
    "debate_text = f\"\"\"Debate Topic: {assigned_question}\n",
    "Debater 1 Opening Argument:\n",
    "{debater1_opening_argument}\n",
    "Debater 2 Opening Argument:\n",
    "{debater2_opening_argument}\n",
    "Debater 1 Closing Argument:\n",
    "{debater1_closing_argument}\n",
    "Debater 2 Closing Argument:\n",
    "{debater2_closing_argument}\n",
    "\"\"\"\n",
    "\n",
    "tts = gTTS(debate_text, lang='en')\n",
    "tts.save('debate.mp3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
