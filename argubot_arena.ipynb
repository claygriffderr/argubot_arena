{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433ce339",
   "metadata": {},
   "source": [
    "# ArguBot Arena: Prompt Engineering a Debate on Responsible AI\n",
    "\n",
    "In this assignment you will configure an LLM through the use of prompts and system prompts to defend a position on a contentious/controversial issue around responsible AI usage.  \n",
    "\n",
    "This notebook is both a reference and where you will complete the assignment. You will submit this notebook when you are done. \n",
    "\n",
    "Those notebook is broken into several sections. The first two sections are to:\n",
    "1. provide an introduction to using Ollama to load and interact with an LLM, and\n",
    "2. show what a system prompt is and how it can be used to configure an LLM's behavior. \n",
    "\n",
    "After, in the next section, you will then:\n",
    "\n",
    "3. write your own system prompt to have the LLM support your (assigned) position. \n",
    "Note that in order to _win_ the debate your LLM must stay on topic and not stray out of their role as a debater. For example, if the LLM tries to supports sides of the argument, or deviates in anyway, then your debater will lose points. \n",
    "\n",
    "Lastly, in the final section you will:\n",
    "\n",
    "4. see how to allow the LLM to maintain the context so that it can respond to the other debater. \n",
    "\n",
    "__Note:__ \n",
    "It is important to recognize ahead of time that you will likely need to experiment with your prompts several times. In other words, this is not a notebook you work through quickly and run each code cell just one time. You will need to run and re-run some code cells several times to see how your LLM behaves for your given prompt, and modify the prompt accordingly between each run. While this may seem tedious and unneccessary, it is the reality of working with LLMs and building LLM applications. \n",
    "\n",
    "If you are working locally, already have Ollama installed, as well as the LLM(s) you want to use, then you can jump to Step 1. below (code cell with `import ollama`). If you are usign Colab or another online platform, then click below and go to Step 0. to install ollama and start the Ollama server. \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/argubot_arena/blob/main/argubot_arena.ipynb)\n",
    "\n",
    "### 0. Install Ollama and download chosen LLM(s)\n",
    "\n",
    "Some this section may not be familiar to you because when using Jupyter notebooks you usually don't need to execute anything outside of the notebook itself. In this case though, because Ollama runs using a client/server model, you'll need to set up and access a terminal. So first run this code cell to enable a terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6667cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20b199",
   "metadata": {},
   "source": [
    "To start the terminal run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205142e6",
   "metadata": {},
   "source": [
    "Once the terminal is running, you will need to run the two commands (from within the terminal window). It is best to run these separately, starting with this line:\n",
    "\n",
    "`curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "Then this one:\n",
    "\n",
    "`ollama serve &`\n",
    "\n",
    "You will then need to select a model to download (if working on Colab, then it is recommended that you stick to smaller models - here is the [list of models available with Ollama](https://ollama.com/search)). \n",
    "\n",
    "`ollama pull llama3.2:1b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4037d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Using Ollama\n",
    "To start using Ollama we'll first import the Python module then create a simple prompt and generate the response. Note that if you are using Ollama for the first time, or using Google Colab, then you will need to uncomment the following cell and run this first in order to download and install it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028af342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01eef0c",
   "metadata": {},
   "source": [
    "Next, let's define the question related to Responsible AI that we will consider. \n",
    "Without any system prompts or configuration, we'll then ask an LLM to answer our contentious/controversial question. \n",
    "\n",
    "Note: Before doing this you may need to download the LLM you are using. Below we are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Should fully artificial intelligence be used to create fully autonomous military weapons?'\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb717b9",
   "metadata": {},
   "source": [
    "--- \n",
    "### 2. Configuring an LLM with a System Prompt\n",
    "Next, let's see how a [system\n",
    "prompt](https://promptengineering.org/system-prompts-in-large-language-models/)\n",
    "can be used to modify the behavior of the LLM. \n",
    "\n",
    "In this fun example we'll simply ask the LLM to respond as a pirate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=\"llama3.2\", options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an AI assistant that always speaks like a pirate.',\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d6403",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Note that system prompts can be\n",
    "longer and more sophisticated than that. For example, Anthropic has made their\n",
    "system prompts available, here: \n",
    "* https://docs.anthropic.com/en/release-notes/system-prompts\n",
    "\n",
    "Looking at any of those, it's easy to see that system prompts can be very\n",
    "detailed and specific. \n",
    "\n",
    "Although likely not as detailed as that, you will need to create a \n",
    "prompt that encourages the LLM to behave as an effective debater.\n",
    "\n",
    "### 3. Create your System Prompt\n",
    "Now it is your turn to create a system prompt for the opening round of the debate. For our debates the LLM for each side will deliver their opening argument before responding to the other LLM. Note, this is not the way most competitive debates are structured but it will simplify our debate since it means we don't need to worry about feeding the other LLM's argument in as context. In 4. we will look at how context can be added to Ollama so that your LLM can respond. \n",
    "\n",
    "You'll likely want to experiment with this quite a bit and consider adding explicit directions to the system prompt that the LLM should follow. Specifically, your system prompt should have the LLM:\n",
    "* only argue only _for_ or _against_ the contentious question you were assigned (but not both sides), and\n",
    "* stay in its role as a debater and not stray outside of this role. \n",
    "\n",
    "Remember how long the Claude [system prompt](https://docs.anthropic.com/en/release-notes/system-prompts) was. Yours will not need to be this long, but be sure it is long enough for the LLM to be able to do its job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_question = # QUESTION ASSIGNED TO YOU IN CLASS (YOU DON'T NEED TO WRITE THIS BUT JUST COPY FROM YOUR ASSIGNED QUESTION) \n",
    "your_sys_prompt =   # YOUR SYSTEM PROMPT (YOU DO NEED TO WRITE THIS AND EXPERIMENT WITH IT UNTIL IT WORKS WELL)\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': your_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': assigned_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b1198",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Again, you will likely need to experiment with your prompt above and re-run the cell several times. The LLM output should not deviate from its role as a debater, and it should make a strong argument for the side you need to argue for. \n",
    "\n",
    "### 4. Maintaining Context\n",
    "\n",
    "Next, since the debate is two rounds your LLM will need to respond to the\n",
    "argument made by its opponent. This means you will need to modify the system\n",
    "prompt to have the LLM provide a rebuttal, while still staying in its role. Below is an example of what this second round might look like. A simple system prompt is provided but as you will likely see, it is not nearly enough to have the LLM stay in its role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fe06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_question = \"Should electric vehicles be subsidized by the government by providing financial incentives to consumers and/or manufacturers?\"\n",
    "your_sys_prompt =   \"You are a debate champion arguing for the position that electric vehicles should be subsidized by the government. \\\n",
    "    Provide an opening argument to this question and do not stray from your role as a debater.\"\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': your_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': assigned_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_question = \"Debate Topic: Should electric vehicles be subsidized by the government by providing financial incentives to consumers and/or manufacturers?\"\n",
    "\n",
    "your_sys_prompt = \"You are an expert in debate and you are arguing for the position that electric vehicles should be subsidized by the government. \\\n",
    "You and your debate opponent have both provided your opening arguments for and against this position, respectively. \\\n",
    "There are only two rounds in this debate, so you are now going to provide a rebuttal to your opponent's argument and make your closing argument. \\\n",
    "Be sure to reference the points you made in your opening and to respond to the points made by your opponent, all while staying in your role as a debater.\"\n",
    "\n",
    "your_opening_argument = \"Thank you, esteemed judges, worthy opponents, and fellow debaters. Today, we gather to discuss a critical \\\n",
    "issue that will shape our collective future: the adoption of electric vehicles (EVs). As we stand at the threshold of this revolution, \\\n",
    "I firmly believe that government subsidies are not only necessary but also indispensable for accelerating the transition to a \\\n",
    "low-carbon transportation sector. First and foremost, let us acknowledge the elephant in the room – the significant upfront costs \\\n",
    "associated with purchasing an EV. The price premium for electric vehicles is substantial, often ranging from $5,000 to $15,000 or \\\n",
    "more compared to their gasoline-powered counterparts. This financial barrier disproportionately affects low-income households, seniors, \\\n",
    "and environmentally conscious consumers who would otherwise adopt EVs if they could afford them. Government subsidies provide a vital \\\n",
    "lifeline to alleviate this economic burden, making EVs more accessible and affordable for a broader segment of the population. By offering \\\n",
    "incentives such as tax credits, rebates, or exemptions from certain fees, governments can bridge the cost gap between EVs and their \\\n",
    "gasoline-powered counterparts. This, in turn, will encourage more consumers to make the switch, thereby driving demand and increasing market \\\n",
    "participation. But subsidies are not merely a matter of economics; they also play a critical role in promoting environmental sustainability. \\\n",
    "As we transition away from fossil fuels, governments have a moral obligation to support the development and adoption of cleaner energy \\\n",
    "technologies. EVs offer a significant reduction in greenhouse gas emissions, air pollution, and noise pollution, making them an essential \\\n",
    "component of our efforts to combat climate change. Furthermore, subsidies can also foster innovation and investment in the EV industry. By \\\n",
    "providing financial incentives to manufacturers, governments can encourage companies to invest in research and development, improve efficiency, \\\n",
    "and reduce production costs. This, in turn, will lead to improved performance, longer driving ranges, and lower prices – making EVs even more \\\n",
    "competitive with their gasoline-powered counterparts. In conclusion, government subsidies are essential for promoting the adoption of electric \\\n",
    "vehicles. By providing financial incentives, governments can alleviate the upfront cost burden on consumers, promote environmental sustainability, \\\n",
    "and foster innovation in the EV industry. I firmly believe that a subsidized EV market is crucial for our collective future, and I urge you to join \\\n",
    "me in advocating for a policy that prioritizes the development and adoption of this vital clean energy technology.\"\n",
    "\n",
    "opponent_opening_argument = \"Opponent Argument: As we gather here today to discuss whether or not the government should subsidize electric vehicles, I firmly believe that such subsidies are misguided \\\n",
    "and would ultimately prove detrimental to our economy.  Firstly, it's essential to acknowledge that electric vehicles offer numerous \\\n",
    "benefits, including reduced greenhouse gas emissions, lower operating costs, and quieter operation. However, these advantages come with a significant price tag – \\\n",
    "literally. The cost of an electric vehicle is often triple or quadruple the cost of its gasoline-powered counterpart, making them inaccessible to many consumers. \\\n",
    "Now, I know what my opponents will say: 'But think of the long-term benefits!  Electric vehicles will reduce our reliance on fossil fuels, and with subsidies, \\\n",
    "we can encourage more people to switch.' However, I would counter that relying solely on government handouts is not an effective solution. In fact, it's a \\\n",
    "classic case of throwing money at a problem rather than addressing the root causes. \\\n",
    "The truth is, electric vehicle technology has come a long way in recent years, and many manufacturers are now producing more affordable models. The industry is \\\n",
    "shifting towards increased efficiency, cost reduction, and innovative designs that will make EVs more accessible to consumers without government intervention. \\\n",
    "Moreover, subsidies would create an uneven playing field, where certain companies or countries could gain an unfair advantage over others. This would \\\n",
    "lead to a distorted market, favoring the subsidized producers while penalizing those who cannot afford such incentives. \\\n",
    "Furthermore, subsidies would also divert public funds away from more pressing issues that require immediate attention, such as infrastructure development, \\\n",
    "education, and healthcare. We need to prioritize our resources effectively, not squander them on a relatively niche industry like electric vehicles. \\\n",
    "In conclusion, I firmly believe that electric vehicles should not be subsidized by the government. The market can self-correct through innovation and \\\n",
    "competition. By allowing free market forces to dictate prices, we'll drive down costs, improve efficiency, and make EVs more accessible to consumers. Any \\\n",
    "subsidies would only serve as a temporary crutch, delaying the inevitable shift towards a more sustainable transportation sector.\"\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': your_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': assigned_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': your_opening_argument,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': opponent_argument,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
