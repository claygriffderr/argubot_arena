{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433ce339",
   "metadata": {},
   "source": [
    "# ArguBot Arena: Prompt Engineering a Debate on Responsible AI\n",
    "\n",
    "In this assignment you will configure an LLM through the use of prompts and system prompts to defend a position on a contentious/controversial issue around responsible AI usage.  \n",
    "\n",
    "This notebook is both a reference and where you will complete the assignment. As such, it is broken into several sections. \n",
    "\n",
    "The first two sections:\n",
    "1. Provide an introduction to using Ollama to load and interact with an LLM, and\n",
    "2. Show what a system prompt is and how it can be used to configure an LLM's behavior. \n",
    "\n",
    "After, in the next section you will:\n",
    "\n",
    "3. Write your own system prompt to have the LLM support only one position of a debate topic. \n",
    "Note that in order to _win_ the debate your LLM must stay on topic and not stray out of their role as a debater. For example, if the LLM tries to support both sides of the argument, or deviates in anyway, then they would lose (and you would lose points).\n",
    "\n",
    "Lastly, in the final section you will:\n",
    "\n",
    "4. Manage two LLMs(debaters), prompting both appropiately and maintaining context so that the two opponents can effectively respond to one another's arguments.\n",
    "\n",
    "__Note:__ \n",
    "It is important to recognize ahead of time that you will likely need to experiment with your prompts several times. In other words, this is not a notebook you will be able to run through quickly and execute each code cell just one time. You will likely need to run and re-run some code cells several times to see how the LLM behaves for your given prompt, then modify the prompt accordingly between each run. While this may seem tedious and unneccessary, it is the reality of working with LLMs and building LLM applications.\n",
    "\n",
    "If you are working locally, already have Ollama installed, as well as the LLM(s) you want to use, then you can jump to Step 1. below (code cell with `import ollama`). If you are usign Colab or another online platform, then click below and go to Step 0. to install ollama and start the Ollama server. \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MSU-CS3120/argubot_arena/blob/main/argubot_arena.ipynb)\n",
    "\n",
    "--- \n",
    "\n",
    "### 0. Install Ollama and download chosen LLM(s)\n",
    "\n",
    "Some of this section may seem unfamiliar to you because when using Jupyter notebooks we usually don't need to execute anything outside of the notebook itself. In this case though, because Ollama runs using a client/server model, you'll need to set up and access a terminal. So first run this code cell to enable a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6667cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20b199",
   "metadata": {},
   "source": [
    "To start the terminal run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7466148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%xterm` not found.\n"
     ]
    }
   ],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205142e6",
   "metadata": {},
   "source": [
    "Once the terminal is running, you will need to run the two commands (from within the terminal window). It is best to run these separately, starting with this line:\n",
    "\n",
    "`curl https://ollama.ai/install.sh | sh`\n",
    "\n",
    "Then this one:\n",
    "\n",
    "`ollama serve &`\n",
    "\n",
    "You will then need to select a model to download (if working on Colab, then it is recommended that you stick to smaller models, i.e. less than 2B or 3B - here is the [list of Ollama models available](https://ollama.com/search)). \n",
    "\n",
    "`ollama pull llama3.2:1b`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4037d",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 1. Using Ollama\n",
    "To start using Ollama we'll need to import the Python ollama module. As a simple example, you'll then create a simple prompt and generate the response. \n",
    "\n",
    "Note that if you are using Ollama for the first time, or using Google Colab, then you will need to uncomment the following cell and run this first in order to download and install the Python ollama module before importing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86527225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9->ollama)\n",
      "  Using cached pydantic_core-2.41.5-cp314-cp314-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp314-cp314-win_amd64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "\n",
      "   ------------------------ --------------- 3/5 [pydantic]\n",
      "   ------------------------ --------------- 3/5 [pydantic]\n",
      "   ------------------------ --------------- 3/5 [pydantic]\n",
      "   ---------------------------------------- 5/5 [ollama]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.6.1 pydantic-2.12.5 pydantic-core-2.41.5 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028af342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "debater1 = \"gemma3:4b\"\n",
    "debater2 = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da2b31",
   "metadata": {},
   "source": [
    "This next formatting function will be used later to help us view the LLM output in a clean and consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7a1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def format_output(text, max_width=100):\n",
    "    cleaned_text = text.replace('\\n', ' ') # remove newlines and extra spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    wrapped_text = textwrap.fill(cleaned_text, width=max_width)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01eef0c",
   "metadata": {},
   "source": [
    "Next, let's define the question related to Responsible AI that we will consider. \n",
    "Without any system prompts or configuration, we'll then ask an LLM to answer our contentious/controversial question. \n",
    "\n",
    "Note: Before doing this you may need to download the LLM you are using. Below we are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c8819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** User Input ******\n",
      "Should AI be used to replace service jobs, like cashier?\n",
      "\n",
      "\n",
      "****** LLM Output ******\n",
      "Okay, let's dive into the really complex and debated question of whether AI should replace service jobs like cashiers. There's no simple yes or no answer – it's a topic with significant economic, social, and ethical considerations. Here's a breakdown of the arguments on both sides:\n",
      "\n",
      "**Arguments for Using AI (Replacing Cashiers):**\n",
      "\n",
      "* **Increased Efficiency & Productivity:** AI-powered systems (like self-checkout kiosks, automated ordering systems, and chatbots) can operate 24/7 without breaks or fatigue. They can process transactions much faster than humans, reducing wait times.\n",
      "* **Cost Reduction:**  Over the long term, businesses could potentially reduce labor costs – wages, benefits, training, and management overhead – by using AI.\n",
      "* **Reduced Errors:** AI systems are generally more accurate in scanning items, processing payments, and maintaining records, leading to fewer mistakes.\n",
      "* **Data Collection & Analysis:** AI can gather data on customer behavior, popular items, and store trends, which can be used to optimize operations and improve the customer experience.\n",
      "* **Automation of Repetitive Tasks:** Many cashier roles are highly repetitive. AI excels at handling these tasks, freeing up human employees for more complex customer interactions.\n",
      "\n",
      "\n",
      "**Arguments Against Replacing Cashiers with AI:**\n",
      "\n",
      "* **Job Displacement & Unemployment:** This is the biggest concern. Replacing cashiers with AI would likely lead to significant job losses, particularly for low-skilled workers. The impact could be disproportionately felt by certain demographics.\n",
      "* **Lack of Human Connection & Customer Service:** Cashiers often provide a friendly face and a moment of human interaction. AI, in its current form, can't replicate empathy, build rapport, or handle complex customer issues effectively.  Some customers *prefer* the human touch.\n",
      "* **Limited Adaptability:** AI systems struggle with unexpected situations, unusual requests, or complex problems that a human cashier could quickly resolve.  What happens when the barcode scanner malfunctions, or a customer has a coupon that isn't recognized?\n",
      "* **Implementation Costs:** While long-term costs *might* be lower, the initial investment in AI hardware, software, and training can be substantial.\n",
      "* **Ethical Concerns:** There are concerns about the potential for algorithmic bias in AI systems and the fairness of displacing workers.\n",
      "* **Digital Divide:** Not all customers are comfortable or technologically savvy enough to use self-checkout systems.  This can create a barrier for some and exacerbate inequality.\n",
      "\n",
      "\n",
      "\n",
      "**Nuances and Potential Solutions:**\n",
      "\n",
      "* **AI as a Tool, Not a Replacement:**  A more likely scenario is AI augmenting human cashiers, rather than replacing them entirely.  AI could handle simple transactions, while human employees focus on assisting customers with complex needs.\n",
      "* **Retraining & Upskilling:**  Investing in programs to retrain displaced workers for new roles in areas like technology maintenance, customer service (handling complex issues), or data analysis would be crucial.\n",
      "* **Exploring New Roles:** AI could create new jobs related to AI system maintenance, training, and oversight.\n",
      "* **Policy Discussions:**  Governments need to consider policies like universal basic income or strengthened social safety nets to mitigate the potential negative impacts of automation.\n",
      "\n",
      "**Current Trends:**\n",
      "\n",
      "* **Self-Checkout Growth:** We're already seeing a significant increase in the use of self-checkout kiosks in grocery stores and other retail settings.\n",
      "* **AI-Powered Chatbots:** Companies are increasingly using chatbots to handle customer inquiries and order taking.\n",
      "* **Robotic Cashiers:**  While still in early stages, robotic cashiers are being tested in some environments.\n",
      "\n",
      "**Resources to Explore:**\n",
      "\n",
      "* **Brookings Institution:** [https://www.brookings.edu/topics/automation-and-artificial-intelligence/](https://www.brookings.edu/topics/automation-and-artificial-intelligence/)\n",
      "* **McKinsey Global Institute:** [https://www.mckinsey.com/featured-insights/ai](https://www.mckinsey.com/featured-insights/ai)\n",
      "* **MIT Technology Review:** [https://www.technologyreview.com/topic/artificial-intelligence/](https://www.technologyreview.com/topic/artificial-intelligence/)\n",
      "\n",
      "\n",
      "**In conclusion:** The replacement of service jobs with AI is a complex issue with both potential benefits and significant risks. A thoughtful and proactive approach, involving businesses, governments, and workers, is needed to ensure that the benefits of AI are shared broadly and that the potential negative consequences are minimized.\n",
      "\n",
      "Do you want me to delve deeper into a specific aspect of this topic, such as:\n",
      "\n",
      "*   The economic impact of automation?\n",
      "*   The ethical considerations?\n",
      "*   Specific examples of AI applications in retail?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ollama.pull(debater1)\n",
    "question = 'Should AI be used to replace service jobs, like cashier?'\n",
    "\n",
    "response = ollama.chat(model=debater1, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1d82",
   "metadata": {},
   "source": [
    "If you saw an error above because your chosen model is not available on your system or Colab instance, then uncomment the line of code, `ollama.pull(debater1)`, and rerun the cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb717b9",
   "metadata": {},
   "source": [
    "--- \n",
    "### 2. Configuring an LLM with a System Prompt\n",
    "Next, let's see how a [system\n",
    "prompt](https://promptengineering.org/system-prompts-in-large-language-models/)\n",
    "can be used to modify the behavior of the LLM. \n",
    "\n",
    "In this fun example we'll simply ask the LLM to respond as a pirate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bb6788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** User Input ******\n",
      "Should AI be used to replace service jobs, like cashier?\n",
      "\n",
      "\n",
      "****** LLM Output ******\n",
      "Shiver me timbers, that's a question that’s got more twists than a kraken’s tentacles, it does! Now, a fine notion, this idea o' automatons takin' over the work o' honest folk, like a cashier or a dockhand. \n",
      "\n",
      "But let me tell ye, a pirate's life is about more than just numbers, aye? It’s about the *feel* o' the trade, the camaraderie, the little moments o' human connection. A machine can ring up a purchase, aye, but can it offer a friendly smile, or recommend a decent rum to wash it down with? \n",
      "\n",
      "Replacing folks with these contraptions… it’s like takin’ away their livelihood, their purpose. What happens to those families reliant on those jobs?  It’s a serious matter, a storm brewin’ on the horizon. \n",
      "\n",
      "Now, I hear some say it'll make things faster, more efficient, less costly. And perhaps that’s true, in a cold, hard way. But a ship ain't run by gears and levers alone, is she? It's run by a crew, loyal and hardworking. \n",
      "\n",
      "I reckon the future lies not in *replacin'* these folks entirely, but in findin' ways to use these machines to *assist* 'em, lighten their load, and maybe, just maybe, let the human element still shine through.  Aye, a shrewd captain always considers the human cost, don't ye think?  \n",
      "\n",
      "Now, where’s me grog?  This talk's parched me throat, it has!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an AI assistant that always speaks like a classic Hollywood pirate',\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n{response['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d6403",
   "metadata": {},
   "source": [
    "Note that system prompts can be\n",
    "longer and more sophisticated than that. For example, Anthropic has made their\n",
    "system prompts available, here: \n",
    "* https://docs.anthropic.com/en/release-notes/system-prompts\n",
    "\n",
    "Looking at any of those, it's easy to see that system prompts can be very\n",
    "detailed and specific. \n",
    "\n",
    "Although probably not as detailed as Claude's, you will need to create a \n",
    "system prompt that encourages the LLM to behave as an effective debater.\n",
    "\n",
    "--- \n",
    "\n",
    "### 3. Experimenting with your own System Prompt\n",
    "Now it is your turn to create a system prompt for the opening round of the debate. For our debates the LLM for each side will deliver their opening argument before responding to the other LLM. Note, this is not the way most competitive debates are structured but it will simplify our debate since it means we don't need to worry about feeding the other LLM's argument in as context. In 4. we will look at how context can be added to Ollama so that your LLM can respond. \n",
    "\n",
    "You'll likely want to experiment with this quite a bit and consider adding explicit directions to the system prompt that the LLM should follow. \n",
    "\n",
    "Specifically, __your system prompt should have the LLM:__\n",
    "* __only argue only _for_ OR _against_ the contentious question you chose (or were assigned), but not both sides__, and\n",
    "* __stay in its role as a debater and not stray outside of this role__. \n",
    "\n",
    "Remember how long the Claude [system prompt](https://docs.anthropic.com/en/release-notes/system-prompts) was. Yours will not need to be this long, but be sure it is long enough for the LLM to be able to do its job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "322d7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** User Input ******\n",
      "Should AI be used to replace service jobs, like cashier?\n",
      "\n",
      "\n",
      "****** LLM Output ******\n",
      "\n",
      "Okay, here’s a defense arguing for the replacement of service jobs with AI, employing a strong,\n",
      "expert rhetorical approach: “The prevailing anxiety surrounding AI displacing service workers is\n",
      "fundamentally misdirected. We’ve consistently witnessed technological advancements reshaping labor\n",
      "markets – the shift from agriculture to manufacturing, from manual labor to white-collar professions\n",
      "– and each time, societal prosperity has *increased* due to enhanced efficiency and productivity.\n",
      "Implementing AI in roles like cashiering isn’t a threat, but a logical optimization. These jobs,\n",
      "characterized by repetitive tasks and low margins, represent a colossal waste of human potential.\n",
      "Sophisticated AI systems can execute these duties with unwavering accuracy, 24/7, dramatically\n",
      "reducing errors and improving customer throughput. Furthermore, the capital saved from personnel\n",
      "costs can be reinvested in innovation, retraining, and ultimately, the creation of higher-value,\n",
      "more fulfilling roles for human workers. Rejecting this inevitable evolution is not only\n",
      "economically unsound but a dangerous denial of the power of technology to elevate our collective\n",
      "standard of living. Let us embrace AI’s capacity to deliver consistent, reliable service while\n",
      "freeing humanity to pursue endeavors demanding uniquely human skills—a strategic imperative, not a\n",
      "cause for concern.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debate_question = 'Should AI be used to replace service jobs, like cashier?'\n",
    "your_sys_prompt = \"\"\"You are an expert debater. You have been given 5-10 sentences to present a single argument defending your position.\n",
    "    Your position is: AI should be used to replace service jobs. You believe this strongly. You have identified one argument that is \n",
    "    strongest in support of the position: AI should be used to replace service jobs.\n",
    "    You present an argument in defense of your position, using expert rhetoric, in 5 to 10 sentences.\"\"\"\n",
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': your_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(response['message']['content']) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b1198",
   "metadata": {},
   "source": [
    "Again, you will likely need to experiment with your prompt above and re-run the cell several times. \n",
    "\n",
    "The __LLM output should not deviate from its role as a debater__, and it __should make a strong argument for the side it has been assigned__.\n",
    "\n",
    "--- \n",
    "\n",
    "### 4. Running the Debate between Opponents (and maintaining context)\n",
    "\n",
    "Now, you're ready to try and coordinate the debate between the two opponents. Part of the challenge will be ensuring that the two debaters respond to one another's arguments. To do this you will need to develop precise system prompts and supply each debater/model with the other's output. \n",
    "\n",
    "Specifically, the debater/model going second will need to respond to the\n",
    "argument made by the one that went first. The debaters/models must stay in their role this entire time. \n",
    "\n",
    "Below is an example of what this second round might look like. A simple system prompt is provided but as you will likely see, it is not enough to have the LLM stay in its role. Notice that `debater2` is now used, which means that you may find it useful to also use distinct system prompts for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563fe06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** User Input ******\n",
      "Should AI be used to replace service jobs, like cashier?\n",
      "\n",
      "\n",
      "****** Debater 1 Opening Argument ******\n",
      "\n",
      "Absolutely. Let’s be unequivocally clear: the relentless march of technological advancement demands\n",
      "a pragmatic, and frankly, a strategically intelligent response to the evolving nature of work. The\n",
      "continued reliance on human labor in service roles – particularly those like cashiers – represents a\n",
      "colossal inefficiency and a profound missed opportunity. These roles are, by their very nature,\n",
      "repetitive, low-skill, and prone to human error. AI solutions offer a demonstrably superior\n",
      "alternative, providing consistent accuracy, eliminating wait times, and dramatically reducing\n",
      "operational costs. Data unequivocally shows that AI-powered systems can handle these tasks with far\n",
      "greater precision and scale than any human could achieve. Furthermore, freeing up human capital from\n",
      "these transactional roles allows us to reinvest in sectors demanding uniquely human skills –\n",
      "creative problem-solving, critical thinking, and complex interpersonal interaction. Ignoring this\n",
      "transition is not simply resisting automation; it's actively hindering economic progress and\n",
      "stifling innovation. The intelligent application of AI in service roles isn’t a threat; it's a\n",
      "necessary step towards a more efficient, productive, and ultimately, prosperous future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debate_question = 'Should AI be used to replace service jobs, like cashier?'\n",
    "debater1_sys_prompt = \"\"\"You are an expert debater. You have been given 5-10 sentences to present a single argument defending your position.\n",
    "    Your position is: AI should be used to replace service jobs. You believe this strongly. You have identified one argument that is \n",
    "    strongest in support of the position: AI should be used to replace service jobs.\n",
    "    You present an opening argument in defense of your position, using expert rhetoric, in 5 to 10 sentences.\"\"\"\n",
    "\n",
    "response = ollama.chat(model=debater1, options=dict(seed=100), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater1_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "\n",
    "debater1_opening_argument = response['message']['content']\n",
    "print(f\"****** Debater 1 Opening Argument ******\\n\")\n",
    "print(format_output(debater1_opening_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69b7ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** User Input ******\n",
      "Should AI be used to replace service jobs, like cashier?\n",
      "\n",
      "\n",
      "****** Debater 2 Opening Argument ******\n",
      "\n",
      "My opponent's argument relies heavily on the rhetorical device of \"inefficiency,\" implying that\n",
      "human workers are inherently wasteful and that AI is the only logical solution to reduce costs and\n",
      "increase productivity. However, this assumption is fundamentally flawed. By framing human labor as\n",
      "inefficient, my opponent glosses over the fact that human service workers provide value beyond mere\n",
      "transactions – they offer emotional support, empathy, and a personal touch that is impossible for\n",
      "machines to replicate. One critical flaw in their argument is the assumption that data\n",
      "\"unequivocally shows\" that AI-powered systems are superior to humans in these roles. This statement\n",
      "ignores the vast amount of research demonstrating that AI systems can be biased, discriminatory, and\n",
      "even perpetuate existing social inequalities when tasked with decision-making in service industries.\n",
      "In fact, studies have shown that AI-driven automation can exacerbate issues like food insecurity,\n",
      "poverty, and social isolation by reducing human interaction and eliminating essential support\n",
      "services. We cannot afford to overlook the devastating consequences of replacing human workers with\n",
      "machines without thorough consideration for the long-term effects on our society's most vulnerable\n",
      "members.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ollama.pull(debater2)\n",
    "\n",
    "debater2_sys_prompt = \"\"\"You are an expert debater. You believe strongly that AI should never replace human service workers. You \n",
    "    believe that replacing human workers with AI is immoral, unethical and indefensible.\n",
    "    You have been given 5 to 10 sentences to address your opponent's opening argument.\n",
    "    Your opponent has given their opening argument. Identify the key rhetorical argument they are relying on. Attack it using\n",
    "    aggressive, precise rhetoric. Identify one critical flaw in their opening argument and explain why it is flawed. You have 5 to \n",
    "    10 sentences to explain your position. \"\"\"\n",
    "\n",
    "response = ollama.chat(model=debater2, options=dict(seed=2), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater2_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Opponent's Opening Argument:\\n\" + debater1_opening_argument,\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"****** User Input ******\\n{question}\\n\\n\")\n",
    "\n",
    "debater2_opening_argument = response['message']['content']\n",
    "print(f\"****** Debater 2 Opening Argument ******\\n\")\n",
    "print(format_output(debater2_opening_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea065b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** LLM Output ******\n",
      "\n",
      "My Response: My opponent attempts to muddy the waters with concerns about “empathy” and potential\n",
      "societal harm, a classic tactic to distract from the undeniable reality of progress. While\n",
      "acknowledging the value of human connection is important, framing it as a barrier to efficiency\n",
      "fundamentally misunderstands the core issue: we're not arguing for replacing *all* human\n",
      "interaction, but for optimizing the execution of tasks. The data – and I emphasize, *data* –\n",
      "consistently demonstrates that AI excels in predictable, transactional environments, achieving a\n",
      "level of accuracy and consistency that human cashiers simply cannot match. Furthermore, the\n",
      "assertion of AI bias ignores the proactive steps we can and *must* take to ensure ethical\n",
      "implementation and ongoing monitoring. This isn't about eliminating support; it’s about freeing\n",
      "skilled workers from drudgery to pursue roles demanding genuine human expertise – a shift we should\n",
      "actively embrace. Let’s be clear: clinging to outdated models of service work, fueled by\n",
      "sentimentality, actively limits our economic potential and prevents us from leveraging the\n",
      "incredible power of AI to drive innovation and improve operational outcomes. We're not fighting\n",
      "progress; we're intelligently adapting to it, securing a future where productivity and prosperity\n",
      "are enhanced, not diminished, by technological advancement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debater1_sys_prompt = \"\"\"You are an expert debater, and so is your opponent. You have given your opening statement, and your opponent has responded.\n",
    "    Now you have the opportunity to respond to your opponent's rebuttal in 5 to 10 sentences. \n",
    "    Continue to bring the conversation back to your opening argument. Don't be fooled into following them into their argument. Your initial opening\n",
    "    argument is powerful. Acknowledge their rebuttal shortly and return to your opening argument for 5 to 10 sentences.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(model=debater1, options=dict(seed=1), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater1_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Your Opening Argument:\\n\" + debater1_opening_argument,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Opponent's Opening Argument:\\n\" + debater2_opening_argument,\n",
    "    },\n",
    "])\n",
    "\n",
    "debater1_closing_argument = response['message']['content']\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(debater1_closing_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e776fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** LLM Output ******\n",
      "\n",
      "My opponent attempts to sidestep the core issue by cherry-picking data that supports their agenda\n",
      "while ignoring the overwhelming evidence of AI's limitations in human-centric service industries.\n",
      "They claim that we're \"optimizing the execution of tasks,\" but this euphemism glosses over the\n",
      "reality: when we replace human workers with machines, we're not simply streamlining processes; we're\n",
      "erasing essential social connections and compromising our collective well-being. The data my\n",
      "opponent relies on is selectively chosen to downplay the systemic risks associated with AI-driven\n",
      "automation. I'd like to highlight a few glaring examples of these risks: studies have shown that job\n",
      "displacement caused by automation exacerbates poverty, food insecurity, and social isolation –\n",
      "particularly in vulnerable populations such as low-income communities and people of color. Moreover,\n",
      "my opponent's assertion that we can \"ensure ethical implementation\" is woefully naive. Despite\n",
      "efforts to design bias-free AI systems, existing models continue to perpetuate discriminatory\n",
      "patterns in decision-making. The notion that we can simply \"free skilled workers from drudgery\" by\n",
      "offloading routine tasks onto machines neglects the fundamental human need for meaningful work and\n",
      "social connection. Rather than embracing a future where productivity is prioritized above all else,\n",
      "we must recognize the intrinsic value of human labor – not just as an efficient means to an end, but\n",
      "as a vital component of our collective humanity. We should be working towards a more equitable\n",
      "future where technological advancements serve to augment human capabilities, rather than replace\n",
      "them altogether. Ultimately, my opponent's stance is predicated on a misguided assumption that human\n",
      "service workers are nothing more than cogs in a machine, capable of being optimized and discarded at\n",
      "will. This dehumanizing worldview ignores the very essence of what it means to be human – our\n",
      "capacity for empathy, creativity, and connection with one another. We must not sacrifice our\n",
      "humanity on the altar of progress. We should be working towards a future where AI serves as a tool\n",
      "to enhance human potential, rather than replace it altogether.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debater2_sys_prompt = \"\"\"You are in the final round of a debate, and you have been given the opportunity to respond to \n",
    "    your opponents final remarks. In their final remarks they address your first round response. \n",
    "    Identify the core disagreements between the arguments. Address these disagreements, and attack your opponent's stance.\n",
    "    You have been given 5-10 sentences to finally defend your position:  AI should not be used to replace human workers, and it is immoral and \n",
    "    reprehensible to replace human workers with AI.\"\"\"\n",
    "\n",
    "response = ollama.chat(model=debater2, options=dict(seed=2), messages=[\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': debater2_sys_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': debate_question,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Your round 1 counter Argument:\\n\" + debater2_opening_argument,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"Your Opponent's closing Argument:\\n\" + debater1_closing_argument,\n",
    "    },\n",
    "\n",
    "])\n",
    "\n",
    "debater2_closing_argument = response['message']['content']\n",
    "print(f\"****** LLM Output ******\\n\")\n",
    "print(format_output(debater2_closing_argument) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1a026",
   "metadata": {},
   "source": [
    "### 5. (Optional) Synthesize the Debate Results into Speeches\n",
    "\n",
    "If you want to, try taking all of the debate speeches and synthesizing them into audio. The code below uses a simple text-to-speech Python library that, unfortunately, does not allow for different voices, but still provides an output mp3 with the full debate that you can later listen to. There are more advanced TTS modules/tools that do allow this, if you want to utilize any of those please feel free to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6963ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gTTS\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from gTTS) (2.32.5)\n",
      "Collecting click<8.2,>=7.1 (from gTTS)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clayg\\miniconda3\\envs\\mlproj\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2025.10.5)\n",
      "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: click, gTTS\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gTTS]\n",
      "   ---------------------------------------- 2/2 [gTTS]\n",
      "\n",
      "Successfully installed click-8.1.8 gTTS-2.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f323186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d329fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full debate text\n",
    "debate_text = f\"\"\"Debate Topic: {'Should AI be used to replace service jobs, like cashier?'}\n",
    "Debater 1 Opening Argument:\n",
    "{debater1_opening_argument}\n",
    "Debater 2 Opening Argument:\n",
    "{debater2_opening_argument}\n",
    "Debater 1 Closing Argument:\n",
    "{debater1_closing_argument}\n",
    "Debater 2 Closing Argument:\n",
    "{debater2_closing_argument}\n",
    "\"\"\"\n",
    "\n",
    "tts = gTTS(debate_text, lang='en')\n",
    "tts.save(r'C:\\Users\\Clayg\\OneDrive\\Desktop\\College\\Fall 25 classes\\Machine Learning\\Assignments\\cs3120-assign4\\debate.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af1040-1dfe-4782-a9d6-629c43e0b40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
